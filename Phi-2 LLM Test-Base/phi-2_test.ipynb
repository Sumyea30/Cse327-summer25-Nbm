{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec103fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\envs\\phi2_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\envs\\phi2_env\\lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 3060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.41s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Phi-2 Response:\n",
      "\n",
      "Explain gravity to a 5-year-old.\n",
      "## INPUT\n",
      "Gravity is a force that pulls everything down to the ground.\n",
      "##OUTPUT\n",
      "Gravity is like a big hug from the earth that makes everything stay on the ground. It is stronger when things are closer together, like when you jump, and weaker when things are farther apart, like when you throw a ball. Gravity is what keeps us from floating away into space.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Optional: redirect Hugging Face model/cache to D:\n",
    "os.environ[\"HF_HOME\"] = \"D:/HF_CACHE\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/HF_CACHE\"\n",
    "\n",
    "# Model ID\n",
    "model_id = \"microsoft/phi-2\"\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if device == \"cuda\" else \"CPU\")\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Explain gravity to a 5-year-old.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=150)\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\n Phi-2 Response:\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c17979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 3060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi-2 Chat is ready. Type 'exit' to stop.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#  Use local cache only\n",
    "os.environ[\"HF_HOME\"] = \"D:/HF_CACHE\"\n",
    "\n",
    "model_id = \"microsoft/phi-2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if device == \"cuda\" else \"CPU\")\n",
    "\n",
    "#  Load model and tokenizer offline\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "#  Chat loop for Jupyter\n",
    "def chat_with_phi2():\n",
    "    print(\"Phi-2 Chat is ready. Type 'exit' to stop.\")\n",
    "    chat_history = \"\"\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower().strip() == \"exit\":\n",
    "            print(\"Chat ended.\")\n",
    "            break\n",
    "\n",
    "        prompt = f\"{chat_history}\\nHuman: {user_input}\\nAssistant:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "        reply = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        reply = reply.split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "        print(f\"\\nPhi-2: {reply}\")\n",
    "        chat_history += f\"\\nHuman: {user_input}\\nAssistant: {reply}\"\n",
    "\n",
    "#  Start the chat\n",
    "chat_with_phi2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f4a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: NVIDIA GeForce RTX 3060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.88s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fadbc090c34eb4a8e57703f2563ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='You:', layout=Layout(width='100%'), placeholder='Type your message...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a044560c7ea4dd9b00138ab096a062c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Set Hugging Face cache\n",
    "os.environ[\"HF_HOME\"] = \"D:/HF_CACHE\"\n",
    "\n",
    "# Load model/tokenizer locally\n",
    "model_id = \"microsoft/phi-2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using:\", torch.cuda.get_device_name(0) if device == \"cuda\" else \"CPU\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# Chat memory\n",
    "chat_history = \"\"\n",
    "\n",
    "# Create widgets\n",
    "input_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your message...',\n",
    "    description='You:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "output_box = widgets.Output()\n",
    "\n",
    "def respond(change):\n",
    "    global chat_history\n",
    "    user_input = change['new']\n",
    "    if user_input.strip().lower() == \"exit\":\n",
    "        input_box.disabled = True\n",
    "        display(Markdown(\"**Chat ended.**\"))\n",
    "        return\n",
    "\n",
    "    prompt = f\"{chat_history}\\nHuman: {user_input}\\nAssistant:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "    reply = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    reply = reply.split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "    with output_box:\n",
    "        display(Markdown(f\"**You:** {user_input}\"))\n",
    "        display(Markdown(f\"**Phi-2:** {reply}\"))\n",
    "\n",
    "    chat_history += f\"\\nHuman: {user_input}\\nAssistant: {reply}\"\n",
    "    input_box.value = ''  # clear input\n",
    "\n",
    "# Attach function\n",
    "input_box.observe(respond, names='value')\n",
    "\n",
    "# Display UI\n",
    "display(input_box)\n",
    "display(output_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30bdac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\envs\\phi2_env\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\envs\\phi2_env\\lib\\site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\envs\\phi2_env\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in d:\\envs\\phi2_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in d:\\envs\\phi2_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in d:\\envs\\phi2_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\envs\\phi2_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\envs\\phi2_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\envs\\phi2_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\envs\\phi2_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in d:\\envs\\phi2_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in d:\\envs\\phi2_env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: wcwidth in d:\\envs\\phi2_env\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\envs\\phi2_env\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\envs\\phi2_env\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\envs\\phi2_env\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in d:\\envs\\phi2_env\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 441.3 kB/s eta 0:00:04\n",
      "   --------- ------------------------------ 0.5/2.2 MB 441.3 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.8/2.2 MB 508.0 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.8/2.2 MB 508.0 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.0/2.2 MB 518.8 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.0/2.2 MB 518.8 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 554.5 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 554.5 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 551.9 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 551.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.8/2.2 MB 585.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 614.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 603.7 kB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   ---------------------------------------- 3/3 [ipywidgets]\n",
      "\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
